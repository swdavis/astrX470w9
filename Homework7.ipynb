{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc46c95-d351-4787-ae77-19adbd0c9f32",
   "metadata": {},
   "source": [
    "# Homework #7\n",
    "\n",
    "This homework will cover a mix of parallelization and running MPI jobs on the cluster.  Hence, some of the problems will require running (short) jobs on Rivanna.\n",
    "\n",
    "**homework7.py** This homework is unusual in that there is no need to create a `homework7.py` file. Instead you will be creating or modifying several files as part of the homework, as described in each problem below.  Changes to tracked files need to be committed and new files added to the repo (e.g. `git add scaling.tab`) and pushed to GitHub.\n",
    "\n",
    "**Group work**:  You do not have to do this as a group, but I would encourage students to perform the scaling test in problem 1 in groups of 2-3 since this will cut down on the jobs run concurrently on the same account.  My worry is that if we all try to run it at once, the jobs may wait in the queue for a long time.  Try to do this problem early in the week to avoid a long wait in the queue!  If you do this as a group, please note who your fellow group members are in the problem below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1efd9-f9cf-4933-8f4a-cd1231771fab",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let's peform a weak scaling test on Rivanna. We will use my favorite code: Athena++. Now, some of you may be assuming that I keep using/referencing Athena++ simply because I am lazy and that is the code that I know how to use the best. While this is correct on both accounts, another reason for choosing Athena++ is its relative lack of dependence on libraries.  This means that it is one of the easier codes to get up and running fast without having to install dependent software.\n",
    "\n",
    "Start by logging into Rivanna.  I should have demonstrated this in class, but the command from a Linux machines would be\n",
    "```\n",
    "ssh -X swd8g@rivanna.hpc.virginia.edu\n",
    "```\n",
    "Obviously, you should replace my username with your username. You should arrive in your home directory.  Recall that you can use the `pwd` command to check your current directory. You should be able clone the public version of athena here\n",
    "```\n",
    "git clone https://github.com/PrincetonUniversity/athena.git\n",
    "```\n",
    "This should create a directory called athena.  Now configure the code to run the linear_wave test in pure hydrodynamics. The sequence of commands for this follows. First, we load the necessary modules for compilign with mpi:\n",
    "```\n",
    "module purge\n",
    "module load gcc\n",
    "module load openmpi\n",
    "module load python\n",
    "```\n",
    "The purge may not be necessary, but it cannot hurt to get rid of any uneeded loaded modules. You can confirm the modules loaded correctly with\n",
    "```\n",
    "module list\n",
    "```\n",
    "You should see a few extra modules that were loaded because openmpi and gcc depend on these. You can also see that there are specific default versions of these modules.\n",
    "\n",
    "Next, we cd into the athena directory, configure the code, and then compile\n",
    "```\n",
    "cd athena\n",
    "python configure.py --prob=linear_wave -mpi\n",
    "make all\n",
    "```\n",
    "The `configure.py` creates a makefile that is then executed by calling `make`.\n",
    "\n",
    "Now make sure you copy over the `slurm.athena` file that is included in this week's repo.  You can do this by cloning the repo to your home directory on Rivanna or using `scp` to copy it from its location (your computer, the Dell server, etc.) to Rivanna.\n",
    "\n",
    "You will want to run the jobs from your scratch directory on Rivanna. For me this will be `/scratch/swd8g` since my user id is `swd8g`. So cd to the corresponding directory for your account and make directories for running the scaling test. Then copy the `athinput.linear_wave3d` and `slurm.athena` codes to this directory.\n",
    "```\n",
    "cd /scratch/swd8g  (change to your directory)\n",
    "mkdir scaling\n",
    "cd scaling\n",
    "mkdir run01\n",
    "mkdir run02\n",
    "mkdir run04\n",
    "mkdir run08\n",
    "mkdir run16\n",
    "mkdir run32\n",
    "mkdir run64\n",
    "cp /home/swd8g/athinput.linear_wave3d .\n",
    "cp /home/swd8g/slurm.athena .\n",
    "```\n",
    "Now proceed with the weak scaling survey. Lets start with a single core. Copy the athinput.linear_wave3d and slurm.athena files into run01. I have written these so that they should run the problem and produce 50 output vtk files. Then launch the scipt\n",
    "```\n",
    "cp athinput.linear_wave3d run01\n",
    "cp slurm.athena run01\n",
    "cd run01\n",
    "sbatch slurm.athena\n",
    "```\n",
    "This will submit your job to the slurm scheduler.  You can check if your job is running by typing\n",
    "```\n",
    "squeue -u swd8g\n",
    "```\n",
    "where you would replace `swd8g` with your username.  If your job is running, it should have an R the status column.  If there not sufficient notes running, it may show up as PD for pending. You should have wait long. If it is running it should be producing files with the \".vtk\" extension. When it finishes there should be a file named `slurm-######.out` where `######` is the job id number.  You can look at this file with the less command\n",
    "```\n",
    "less slurm-######.out\n",
    "```\n",
    "which show the standard diagnostic output of Athena++. This should include information about each timestep and then at the end some assessment of the codes performance. To just look at the last few lines, type\n",
    "```\n",
    "tail slurm-######.out\n",
    "```\n",
    "In my case the last three lines are\n",
    "```\n",
    "zone-cycles = 23330816\n",
    "cpu time used  = 1.2722564999999999e+01\n",
    "zone-cycles/cpu_second = 1.8338138575043634e+06\n",
    "```\n",
    "This last number is the key metric for performance -- it is how many simulation cells get updated per cpu-second for the entire job.  You want to record this number in a file 'scaling.tab` with two columns -- the first number is number of cores used and the second number is zone-cycles/cpu_second.\n",
    "\n",
    "Now, change to the next directory `run02` and copy the `slurm.athena` and `athinput.linear_wave3d'\n",
    "```\n",
    "cd ../run02\n",
    "cp ../run01/athinput.linear_wave3d .\n",
    "cp ../run02/slurm.athena .\n",
    "```\n",
    "Now edit the `slurm.athena` script to change the following lines;\n",
    "```\n",
    "#SBATCH --ntasks=2\n",
    "#SBATCH -J wkath2\n",
    "```\n",
    "Then edit the `athinput.linear_wave3d` to change\n",
    "```\n",
    "nx1        = 128\n",
    "```\n",
    "This doubles the size of the job and number of cores used. Now submit the new job\n",
    "```\n",
    "sbatch slurm.athena\n",
    "```\n",
    "Continue this for each runs, doubling `n` in the  `#SBATCH --ntasks=n` line of the script and changing the job name so that you can tell which is which in the queue.  Each time you will want to double the size of the job by increaseing `nx1`, `nx2` and `nx3` in the `athinput.linear_wave3d` file. I reccomdend increasing these in order: for `run04` keep `nx1=128` and `nx3=32` but double `nx2` so that it equals 64. For `run08` keep `nx1=128` and `nx2=64` but make `nx3=64. Then double `nx1` for the next run and so on.\n",
    "\n",
    "Rivanna's primary nodes only have 20 cores.  This means that when you go from 16 to 32 processes, you will need to increase the nodes to 2 and change to the parallel queue:\n",
    "```\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks=32\n",
    "#SBATCH --partition=parallel\n",
    "```\n",
    "Continue this up to 64 core runs to complete `scaling.tab`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a3ca1-a8ac-48a9-90b3-072e3521b170",
   "metadata": {},
   "source": [
    "Now plot the weak scaling efficiency.  Often this printed relative to single core performance so we will divide by the performance with `--ntasks=1`. One usually expect efficiency to drop as the number of processes increases.  For a single node (N < 20 on Rivanna), you often see efficiency drop due to increase shared memory usage as the number of active cores increases, but this depends on the queuing system and whether or not other users have access to the same node.\n",
    "\n",
    "For jobs exceeding a single node, one often measures efficiency on multiple (full) nodes relative to the performance on a single node.\n",
    "\n",
    "Our scaling test is not entirely optimal because we are not using full nodes, but you should genreally see efficiency decline as the number of processes increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "66edff6d-49b2-4f71-b205-dac048ae9c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tab = np.loadtxt(\"scaling.tab\")\n",
    "\n",
    "nproc = tab[:,0]\n",
    "effic = tab[:,1]/tab[:,0]/tab[0,1] # normalize to single node\n",
    "\n",
    "plt.plot(nproc, effic, 'o')\n",
    "plt.ylabel('Efficiency')\n",
    "plt.xlabel('N')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1ecd0-2274-4189-aa06-b1fe6689eb46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "If you did the test as a group, list the other group members here:\n",
    "\n",
    "Add `scaling.tab` to the git repo using `git add scaling.tab`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ef31a-a302-471d-8560-3171fd50e7bf",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "For this problem we revisit our solution of Poisson's equation using Jacobi iteration. In our notebook, we implemented this using blocking sends and receives. Now try implementing with non-blocking sends and receives by updating the file `jacobi_nb.py`. In particularly, we will attemp to interleave communication and computation by initiating the send and receive calls *before* our call to the `jacobi()` function.  We only need to compute the endpoints of the local grid (`phi[1]` and `phi[-2]`) to pass to neighboring processes so we simply compute these first and then send it before doing the full problem by calling `jacobi()`. Will this improve our performance?  First comput the case with one process -- this should work without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "156782b3-44e2-41ba-a4b8-f73f9f92a451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iteration: 17677 9.999e-08\n",
      "Time with 1 processors: 2.748570e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '1', 'python', 'jacobi_nb.py'], returncode=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"mpirun\",\"-np\",\"1\",\"python\",\"jacobi_nb.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27e5f9-8b9a-4183-a840-cfb7a3df6586",
   "metadata": {},
   "source": [
    "This should take the same number of iterations as in the noteboock and will produce a file called `jacobi_nb.png` that should look like identical to the notebook plot.  Now modify the `main()` fucntion to work with multiple processes and try four processes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "57b5673a-eb50-4603-ae37-05f5ebb130bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iteration: 1303 9.899e-08\n",
      "Time with 4 processors: 3.656400e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'jacobi_nb.py'], returncode=0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"jacobi_nb.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bfc9a-696f-497e-ae8b-7a98d5b5ca3f",
   "metadata": {},
   "source": [
    "If you did this correctly, it should again take the exact same number of iterations and procude an identical plot to what you obtain with a single process. Unfortunately, initiating the communication before the call to the `jacobi()` function didn't help much here.  It is simply the case that (on most machines) the communication overhead (latency) is much larger than the cost to perform `jacobi()` for a problem of this size. This was not a problem worth parallelizing for Python outside of pedagogical motivations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961d663-e973-4eb5-a836-978650374723",
   "metadata": {},
   "source": [
    "## Problem 3 (grads only)\n",
    "\n",
    "Recall that when we first implemented iterative solutions of matrix equations, we discussed both the Jacobi method and the Gauss-Seidel method, with Gauss-Seidel being faster because it incorporates new information.  At the time I mentioned that Jacobi is sometime still used because it parallelizes more efficiently.  The problem with Gauss-Seidel is that you need to incorporate new information (which may be on the other process) to make it converge more efficiently.\n",
    "\n",
    "In order to get around this, we can use a method called Red-Black Gauss-Seidel.  This is a two step method, where you only update every other cell/point in the grid on each substep. For example in 1D, you would first update $i=0,2,4,\\dots$ and then on the next step $i=1,3,5,\\dots$.  In 2D this looks like a checkerboard patter -- hence the name red-black Guass-Seidel.\n",
    "\n",
    "Let's try this on our current 1D problem. Since non-blocking communication didn't help much in problem 2, lets stick to blocking communication.  Below is an example of the Red-black Gauss-Seidel iteration that gives us some practice with NumPy slicing gymanstics.  Recall that the general slice is `begin:end:step`. Something like `3::2` means begin with 3 and take a step of 2. The blank between the two colons means no end is set (i.e. go until the end of the array). If no step is specified, it is set to 1. Here we are using steps of 2 to implement the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd1895-a023-4e9f-a40b-6af25495fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussseidel_redblack(phi, x, rho):\n",
    "    # red\n",
    "    dx = 0.5*(x[3::2] - x[1:-2:2])\n",
    "    phi[2:-1:2] = 0.5*(phi[1:-2:2] + phi[3::2] - dx**2*rho[2:-1:2])\n",
    "    # black\n",
    "    dx = 0.5*(x[2::2] - x[:-2:2])\n",
    "    phi[1:-1:2] = 0.5*(phi[:-2:2]  + phi[2::2] - dx**2*rho[1:-1:2])\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98540264-22ad-4f15-91c8-d6e6916074c5",
   "metadata": {},
   "source": [
    "### Part a\n",
    "\n",
    "Before you implement the MPI changes, begin by implementing the `gaussseidel_red()` and `gaussseidel_black()` functions and test it on a single process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a067932f-d8ba-48bc-940c-0acf54d887a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iteration: 9300 9.994e-08\n",
      "Time with 1 processors: 2.111800e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '1', 'python', 'gaussseidel.py'], returncode=0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"mpirun\",\"-np\",\"1\",\"python\",\"gaussseidel.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4932cc-a4df-4566-a78a-3b227dfaf46e",
   "metadata": {},
   "source": [
    "If you did it correctly, it should only take 9300 iterations -- considerably less than what we found for the Jacobi method. It should also produce a figure `gaussseidel.png` where the converged solution matches the analytics result.\n",
    "\n",
    "### Part b\n",
    "\n",
    "Now implement the MPI communication following the previous examples. The only trick bit here is that `gaussseidel_black()` requires the updates from `gaussseidel_red()` so we need to perform one set of send/receive in-between the functions calls. You need to figure out which one yourself. Try it with four processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cc4b1e8d-4f97-48f6-84f4-8c2f7e31a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iteration: 683 9.867e-08\n",
      "Time with 4 processors: 2.369400e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'gaussseidel.py'], returncode=0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"gaussseidel.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104bdd6-0e2b-4266-bbe5-c1e4323901bd",
   "metadata": {},
   "source": [
    "If you did it correctly, it should take the same number of iterations and the plotted solution should match the analtyics result.  Still slower -- we haven't actually increased the amount of work to be done so communication continues to dominate the cost of the calculation. Sad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9c4e3-622e-4eb2-ac02-b4a5f22528af",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Now lets try writing our own OpenMP code and running it on the Rivanna cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ff0af-432b-47d3-971b-e776aeef9d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc46c95-d351-4787-ae77-19adbd0c9f32",
   "metadata": {},
   "source": [
    "# Homework #7\n",
    "\n",
    "This homework will cover a mix of parallelization and running MPI jobs on the cluster.  Hence, some of the problems will require running (short) jobs on Rivanna.\n",
    "\n",
    "**Group work**:  You do not have to do this as a group, but I would encourage students to perform the scaling test in problem 1 in groups of 2-3 since this will cut down on the jobs run concurrently on the same account.  My worry is that if we all try to run it at once, the jobs may wait in the queue for a long time.  Try to do this problem early in the week to avoid a long wait in the queue!  If you do this as a group, please note who your fellow group members are in the problem below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ce3288f-0360-4a1f-a85a-b30e019ae1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load homework\n",
    "import homework7 as hw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d4088-273c-48c9-a286-53acd8b06d6b",
   "metadata": {},
   "source": [
    "For performance purposes, Python ignores requests to reimport a module after it has been imported once.  This behavior is not ideal if you are actively developing the module.  You could reset the kernel via the Jupyter lab interface, but the simpler solution is to use the functionality of the `importlib` module as follows.  You can call this anytime you modify homework1.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be776c1c-6a14-4604-9c92-464bd0702685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'homework7' from '/Users/swd8g/Box Sync/astrX470/weeks/w9/homework7.py'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload module if already loaded\n",
    "import importlib\n",
    "importlib.reload(hw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1efd9-f9cf-4933-8f4a-cd1231771fab",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let's peform a weak scaling test on Rivanna. We will use my favorite code: Athena++. Now, some of you may be assuming that I keep using/referencing Athena++ simply because I am lazy and that is the code that I know how to use the best. While this is correct on both accounts, another reason for choosing Athena++ is its relative lack of dependence on libraries.  This means that it is one of the easier codes to get up and running fast without having to install dependent software.\n",
    "\n",
    "Start by logging into Rivanna.  I should have demonstrated this in class, but the command from a Linux machines would be\n",
    "```\n",
    "ssh -X swd8g@rivanna.hpc.virginia.edu\n",
    "```\n",
    "Obviously, you should replace my username with your username. You should arrive in your home directory.  Recall that you can use the `pwd` command to check your current directory. You should be able clone the public version of athena here\n",
    "```\n",
    "git clone https://github.com/PrincetonUniversity/athena.git\n",
    "```\n",
    "This should create a directory called athena.  Now configure the code to run the linear_wave test in pure hydrodynamics. The sequence of commands for this follows. First, we load the necessary modules for compilign with mpi:\n",
    "```\n",
    "module purge\n",
    "module load gcc\n",
    "module load openmpi\n",
    "```\n",
    "The purge may not be necessary, but it cannot hurt to get rid of any uneeded loaded modules. You can confirm the modules loaded correctly with\n",
    "```\n",
    "module list\n",
    "```\n",
    "You should see a few extra modules that were loaded because openmpi and gcc depend on these. You can also see that there are specific default versions of these modules.\n",
    "\n",
    "Next, we cd into the athena directory, configure the code, and then compile\n",
    "```\n",
    "cd athena\n",
    "python configure.py --prob=linear_wave -mpi\n",
    "make all\n",
    "```\n",
    "The `configure.py` creates a makefile that is then executed by calling `make`. You can speed the process up by using `make -j 8 all`, which will spread the compilation accross multiple processes -- eight in this example.\n",
    "\n",
    "Now make sure you copy over the `slurm.athena` file that is included in this week's repo.  You can do this by cloning the repo to your home directory on Rivanna or using `scp` to copy it from its location (your computer, the Dell server, etc.) to Rivanna.\n",
    "\n",
    "You will want to run the jobs from your scratch directory on Rivanna. For me this will be `/scratch/swd8g` since my user id is `swd8g`. So cd to the corresponding directory for your account and make directories for running the scaling test. Then copy the `athinput.linear_wave3d` and `slurm.athena` codes to this directory.\n",
    "```\n",
    "cd /scratch/swd8g  (change to your directory)\n",
    "mkdir scaling\n",
    "cd scaling\n",
    "mkdir run01\n",
    "mkdir run05\n",
    "mkdir run10\n",
    "mkdir run20\n",
    "mkdir run40\n",
    "cp /home/swd8g/athena/inputs/hydro/athinput.linear_wave3d .\n",
    "cp /home/swd8g/slurm.athena .\n",
    "```\n",
    "Now copy the athinput file and slurm.athena script into each of the run directories.\n",
    "\n",
    "These jobs should not have to run for very long so \n",
    "\n",
    "reate a script for each run and launch the scripts simultaneously or "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1ecd0-2274-4189-aa06-b1fe6689eb46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "If you did the test as a group, list the other group members here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3c0401f-90c7-4b68-8989-26f86561f9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b06ef31a-a302-471d-8560-3171fd50e7bf",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "For this problem we revisit our solution of Poisson's equation using Jacobi iteration. In our notebook, we implemented this using blocking sends and receives. Now try implementing with non-blocking sends and receives by updating the file `jacobi_nb.py`. In particularly, we will attemp to interleave communication and computation by initiating the send and receive calls *before* our call to the `jacobi()` function.  We only need to compute the endpoints of the local grid (`phi[1]` and `phi[-2]`) to pass to neighboring processes so we simply compute these first and then send it before doing the full problem by calling `jacobi()`. Will this improve our performance?  First comput the case with one process -- this should work without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "156782b3-44e2-41ba-a4b8-f73f9f92a451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iteration: 17677 9.999e-08\n",
      "Time with 1 processors: 2.748570e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '1', 'python', 'jacobi_nb.py'], returncode=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"mpirun\",\"-np\",\"1\",\"python\",\"jacobi_nb.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27e5f9-8b9a-4183-a840-cfb7a3df6586",
   "metadata": {},
   "source": [
    "This should take the same number of iterations as in the noteboock and will produce a file called `jacobi_nb.png` that should look like identical to the notebook plot.  Now modify the `main()` fucntion to work with multiple processes and try four processes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "57b5673a-eb50-4603-ae37-05f5ebb130bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iteration: 1303 9.899e-08\n",
      "Time with 4 processors: 3.656400e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'jacobi_nb.py'], returncode=0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"jacobi_nb.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bfc9a-696f-497e-ae8b-7a98d5b5ca3f",
   "metadata": {},
   "source": [
    "If you did this correctly, it should again take the exact same number of iterations and procude an identical plot to what you obtain with a single process. Unfortunately, initiating the communication before the call to the `jacobi()` function didn't help much here.  It is simply the case that (on most machines) the communication overhead (latency) is much larger than the cost to perform `jacobi()` for a problem of this size. This was not a problem worth parallelizing for Python outside of pedagogical motivations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961d663-e973-4eb5-a836-978650374723",
   "metadata": {},
   "source": [
    "## Problem 3 (grads only)\n",
    "\n",
    "Recall that when we first implemented iterative solutions of matrix equations, we discussed both the Jacobi method and the Gauss-Seidel method, with Gauss-Seidel being faster because it incorporates new information.  At the time I mentioned that Jacobi is sometime still used because it parallelizes more efficiently.  The problem with Gauss-Seidel is that you need to incorporate new information (which may be on the other process) to make it converge more efficiently.\n",
    "\n",
    "In order to get around this, we can use a method called Red-Black Gauss-Seidel.  This is a two step method, where you only update every other cell/point in the grid on each substep. For example in 1D, you would first update $i=0,2,4,\\dots$ and then on the next step $i=1,3,5,\\dots$.  In 2D this looks like a checkerboard patter -- hence the name red-black Guass-Seidel.\n",
    "\n",
    "Let's try this on our current 1D problem. Since non-blocking communication didn't help much in problem 2, lets stick to blocking communication.  Below is an example of the Red-black Gauss-Seidel iteration that gives us some practice with NumPy slicing gymanstics.  Recall that the general slice is `begin:end:step`. Something like `3::2` means begin with 3 and take a step of 2. The blank between the two colons means no end is set (i.e. go until the end of the array). If no step is specified, it is set to 1. Here we are using steps of 2 to implement the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd1895-a023-4e9f-a40b-6af25495fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussseidel_redblack(phi, x, rho):\n",
    "    # red\n",
    "    dx = 0.5*(x[3::2] - x[1:-2:2])\n",
    "    phi[2:-1:2] = 0.5*(phi[1:-2:2] + phi[3::2] - dx**2*rho[2:-1:2])\n",
    "    # black\n",
    "    dx = 0.5*(x[2::2] - x[:-2:2])\n",
    "    phi[1:-1:2] = 0.5*(phi[:-2:2]  + phi[2::2] - dx**2*rho[1:-1:2])\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98540264-22ad-4f15-91c8-d6e6916074c5",
   "metadata": {},
   "source": [
    "### Part a\n",
    "\n",
    "Before you implement the MPI changes, begin by implementing the `gaussseidel_red()` and `gaussseidel_black()` functions and test it on a single process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a067932f-d8ba-48bc-940c-0acf54d887a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iteration: 9300 9.994e-08\n",
      "Time with 1 processors: 2.111800e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '1', 'python', 'gaussseidel.py'], returncode=0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"mpirun\",\"-np\",\"1\",\"python\",\"gaussseidel.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4932cc-a4df-4566-a78a-3b227dfaf46e",
   "metadata": {},
   "source": [
    "If you did it correctly, it should only take 9300 iterations -- considerably less than what we found for the Jacobi method. It should also produce a figure `gaussseidel.png` where the converged solution matches the analytics result.\n",
    "\n",
    "### Part b\n",
    "\n",
    "Now implement the MPI communication following the previous examples. The only trick bit here is that `gaussseidel_black()` requires the updates from `gaussseidel_red()` so we need to perform one set of send/receive in-between the functions calls. You need to figure out which one yourself. Try it with four processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cc4b1e8d-4f97-48f6-84f4-8c2f7e31a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iteration: 683 9.867e-08\n",
      "Time with 4 processors: 2.369400e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'gaussseidel.py'], returncode=0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"gaussseidel.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104bdd6-0e2b-4266-bbe5-c1e4323901bd",
   "metadata": {},
   "source": [
    "If you did it correctly, it should take the same number of iterations and the plotted solution should match the analtyics result.  Still slower -- we haven't actually increased the amount of work to be done so communication continues to dominate the cost of the calculation. Sad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9c4e3-622e-4eb2-ac02-b4a5f22528af",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ff0af-432b-47d3-971b-e776aeef9d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

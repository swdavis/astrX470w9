{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization\n",
    "\n",
    "This weeks notebook is a bit different.  For many of the Python topics, one can utilize Jupyter directly, but this is not optimal for parallelization.  There is a mechanism for running MPI from IPython notebooks via the `ipyparallel` module but I find it clunky to use. If you are interested, you can check it out here: https://ipyparallel.readthedocs.io/en/latest/\n",
    "\n",
    "For this reason, I have provide several example Python scripts and C codes for you to run, with some markdown comments in the notebook itself.  Also, these will require having MPI and OpenMP working on your machine.  You can install MPI on your laptop. OpenMP may work, but most likely you will need to enable it (e.g. via Xcode on Mac) but my recommendation is to run these on the Dell server. I have already tested that these codes work there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using mpi4py\n",
    "\n",
    "As noted above, `mpi4py` works best with scripts run from the command line and we will proceed with that strategy here.  That said, the scripts can run (serially) in the Jupyter notebook so I copy them below for you to look at and run (if desired).\n",
    "\n",
    "Let's look at a basic example of an mpi code in the file `mpi_example1.py`.  We can run this in the notebook, but it will only correspond to one process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! My rank:  0\n",
      "Number of processes:  1\n"
     ]
    }
   ],
   "source": [
    "# mpi_example1.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    nprocs = comm.Get_size()\n",
    "    my_rank = comm.Get_rank()\n",
    "    print(\"Hello world! My rank: \", my_rank)\n",
    "    if my_rank == 0:\n",
    "        print(\"Number of processes: \", nprocs)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in constrast to C we did not need to initialize or finalize MPI. The `mpi4py` automatically handles this. We did, however, have to set a communicator with:\n",
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "````\n",
    "The functions `Get_size()` and `Get_rank()`, as well as those for sending and receiving, etc. are then methods of `comm` instance that is returned.\n",
    "\n",
    "Running this as a script from the command line allows us to run with mulitple processes.  There are two ways to run this command:\n",
    "* `mpirun -n 4 python mpi_example1.py`\n",
    "* `mpiexec -n 4 python mpi_example1.py`\n",
    "\n",
    "One can also use either `-n` or `-np`. The number 4 means to use four processes. You can run these from the command line or we can use `subprocess.run()` from this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! My rank:  2\n",
      "Hello world! My rank:  3\n",
      "Hello world! My rank:  0\n",
      "Number of processes:  4\n",
      "Hello world! My rank:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'mpi_example1.py'], returncode=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"mpi_example1.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the order in which the processes execute is generally random. We can use the `Barrier()` function to ensure the `Number of procceses` line prints last:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpi_example2.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    nprocs = comm.Get_size()\n",
    "    my_rank = comm.Get_rank()\n",
    "    print(\"Hello world! My rank: \", my_rank)\n",
    "\n",
    "    # ensure that this executes after each rank has completed\n",
    "    # printing Hello World\n",
    "    comm.Barrier()\n",
    "    if my_rank == 0:\n",
    "        print(\"Number of processes: \", nprocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! My rank:  0\n",
      "Hello world! My rank:  2\n",
      "Hello world! My rank:  3\n",
      "Hello world! My rank:  1\n",
      "Number of processes:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'mpi_example2.py'], returncode=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"mpi_example2.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next consider a case where we want to break up some work across multiple processes. If you look at it, this is really fork-join parallelization so this problem is well suited to OpenMP parallelization on a shared memory machine.  Compare the implementations in `mpi_example3.c` and `omp_example3.c` to see how much simpler the OpenMP implementation is the C codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpi_exampe3.py\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "def main():\n",
    "\n",
    "    # get the MPI communicator\n",
    "    comm = MPI.COMM_WORLD\n",
    "    nprocs = comm.Get_size()\n",
    "    my_rank = comm.Get_rank()\n",
    " \n",
    "    N = 10000000\n",
    "\n",
    "    # determine the part of the work done by each rank\n",
    "    npart = [ N // nprocs for i in range(nprocs) ]\n",
    "    # add any extra if not precisely divisible\n",
    "    for i in range(N % nprocs):\n",
    "        npart[i] += 1\n",
    "\n",
    "    my_beg = 0\n",
    "    for i in range(my_rank):\n",
    "        my_beg += npart[i]\n",
    "\n",
    "    # initialize a\n",
    "    start_time = MPI.Wtime()\n",
    "    a = np.empty(npart[my_rank])\n",
    "    for i in range(npart[my_rank]):\n",
    "        a[i] = 2*(i + my_beg)\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Time to initialize a: \",end_time-start_time)\n",
    "\n",
    "    # initialize b\n",
    "    start_time = MPI.Wtime()\n",
    "    b = np.empty(npart[my_rank])\n",
    "    for i in range(npart[my_rank]):\n",
    "        b[i] = (i + my_beg)**2\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Time to initialize b:  \",end_time-start_time)\n",
    "\n",
    "    # add the two arrays in array c\n",
    "    c = np.empty(npart[my_rank])\n",
    "    start_time = MPI.Wtime()\n",
    "    for i in range(npart[my_rank]):\n",
    "        c[i] = a[i] + b[i]\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Time to add arrays: \",end_time-start_time)\n",
    "\n",
    "    # average the result\n",
    "    start_time = MPI.Wtime()\n",
    "    sum = 0\n",
    "    for i in range(npart[my_rank]):\n",
    "        sum += c[i]\n",
    "    local_sum = np.array([sum])\n",
    "    global_sum = np.zeros(1)\n",
    "    comm.Reduce(sum, global_sum, op=MPI.SUM, root=0)\n",
    "    average = global_sum / N\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Average: \",average[0])\n",
    "        print(\"Time to compute average: \",end_time-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 1 process:\n",
      "Time to initialize a:  1.620739\n",
      "Time to initialize b:   4.527000999999999\n",
      "Time to add arrays:  3.004886\n",
      "Average:  33333338333349.945\n",
      "Time to compute average:  1.375375\n",
      "Running with 4 processes:\n",
      "Time to initialize a:  0.6238790000000001\n",
      "Time to initialize b:   1.7651800000000002\n",
      "Time to add arrays:  1.146757\n",
      "Average:  33333338333346.223\n",
      "Time to compute average:  0.5933460000000004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'mpi_example3.py'], returncode=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Running with 1 process:\")\n",
    "subprocess.run([\"mpirun\",\"-np\",\"1\",\"python\",\"mpi_example3.py\"])\n",
    "print(\"Running with 4 processes:\")\n",
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"mpi_example3.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a notable improvement due to the parallelization in this case because the only communication required is a reduce function at the end and we gave each process large components of the arrays to initialize and sum.\n",
    "\n",
    "Next, lets try an example with point-to-point communications. Lets use blocking send and receives first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpi_example4.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    \n",
    "    comm = MPI.COMM_WORLD\n",
    "    my_rank = comm.Get_rank()\n",
    "    nprocs = comm.Get_size()\n",
    "\n",
    "    prev = my_rank - 1\n",
    "    next = my_rank + 1\n",
    "\n",
    "    if my_rank == 0:\n",
    "        prev = nprocs - 1\n",
    "    if my_rank == nprocs - 1:\n",
    "        next = 0\n",
    "\n",
    "    rbuf1 = np.empty(1, dtype=int)\n",
    "    rbuf2 = np.empty(1, dtype=int)\n",
    "    sbuf = np.array([my_rank], dtype=int)\n",
    "    \n",
    "    tag1 = 1\n",
    "    tag2 = 2\n",
    "\n",
    "    # blocking sends\n",
    "    comm.Send([sbuf, MPI.INT], dest=prev, tag=tag2)\n",
    "    comm.Send([sbuf, MPI.INT], dest=next, tag=tag1)\n",
    "    # blocking receives\n",
    "    comm.Recv([rbuf1, MPI.INT], source=prev, tag=tag1)\n",
    "    comm.Recv([rbuf2, MPI.INT], source=next, tag=tag2)\n",
    "\n",
    "    print(\"My rank: {:d} and my neighbors: {:d} {:d}\".format(my_rank, rbuf1[0], rbuf2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My rank: 2 and my neighbors: 1 3\n",
      "My rank: 1 and my neighbors: 0 2\n",
      "My rank: 0 and my neighbors: 3 1\n",
      "My rank: 3 and my neighbors: 2 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'mpi_example4.py'], returncode=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"mpi_example4.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have the sends execute first, then the receives. What happens if we reverse the order and put the receives first?  Also, note that we used NumPy arrays for the buffers.  `mpi4py` requires the buffers to be contiguous memory blocks. That might be the case for other data types (e.g. lists), but the implementation of NumPy ensures this. NumPy arrays are usually used for send/receive buffers with `mpi4py`.\n",
    "\n",
    "Note the use tags as well. In some implementations there may be multiple different buffers communicated between the same pair of processes. The tags help to differentiate these communications so that the receiving buffer knows where to place the different buffers that get sent to it.\n",
    "\n",
    "Now lets look at what we get if we reverse the send and receive order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpi_example5.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    \n",
    "    comm = MPI.COMM_WORLD\n",
    "    my_rank = comm.Get_rank()\n",
    "    nprocs = comm.Get_size()\n",
    "\n",
    "    prev = my_rank - 1\n",
    "    next = my_rank + 1\n",
    "\n",
    "    if my_rank == 0:\n",
    "        prev = nprocs - 1\n",
    "    if my_rank == nprocs - 1:\n",
    "        next = 0\n",
    "\n",
    "    rbuf1 = np.empty(1, dtype=int)\n",
    "    rbuf2 = np.empty(1, dtype=int)\n",
    "    sbuf = np.array([my_rank], dtype=int)\n",
    "    \n",
    "    tag1 = 1\n",
    "    tag2 = 2\n",
    "\n",
    "    # blocking receives\n",
    "    comm.Recv([rbuf1, MPI.INT], source=prev, tag=tag1)\n",
    "    comm.Recv([rbuf2, MPI.INT], source=next, tag=tag2)\n",
    "    # blocking sends\n",
    "    comm.Send([sbuf, MPI.INT], dest=prev, tag=tag2)\n",
    "    comm.Send([sbuf, MPI.INT], dest=next, tag=tag1)\n",
    "\n",
    "    print(\"My rank: {:d} and my neighbors: {:d} {:d}\".format(my_rank, rbuf1[0], rbuf2[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmpirun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-np\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmpi_example5.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:491\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    493\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:1016\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1014\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1016\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:1079\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1079\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:1804\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1804\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:1762\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1762\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1764\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if you try this, it will just hang because none of the receives\n",
    "# will complete as no sends can begin until the receives compelte\n",
    "\n",
    "# subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"mpi_example5.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. If you know anything about MPI communication, putting blocking receives before the sends is obviously dumb. There are, however, cases where the communication gets more complicated and one can end up in a similar position due to some misunderstood subtlety of execution.\n",
    "\n",
    "Now lets try the same thing with non-blocking sends and receives.  The non-blocking versions of `Send()` and `Recv()` are `Isend()` and `Irecv()`. Notice that the syntax of the functions is slightly different.  The `Isend()` and `Irecv()` function return MPI request objects while `Send()` and `Recv()` have no return type.  The MPI request stored in the list `reqs` are used by the `Request.Waitall()` function to confirm that all communication has completed before continuing execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    \n",
    "    comm = MPI.COMM_WORLD\n",
    "    my_rank = comm.Get_rank()\n",
    "    nprocs = comm.Get_size()\n",
    "\n",
    "    prev = my_rank - 1\n",
    "    next = my_rank + 1\n",
    "\n",
    "    if my_rank == 0:\n",
    "        prev = nprocs - 1\n",
    "    if my_rank == nprocs - 1:\n",
    "        next = 0\n",
    "\n",
    "    rbuf1 = np.empty(1, dtype=int)\n",
    "    rbuf2 = np.empty(1, dtype=int)\n",
    "    sbuf = np.array([my_rank], dtype=int)\n",
    "    \n",
    "    tag1 = 1\n",
    "    tag2 = 2\n",
    "    reqs = [None] * 4\n",
    "\n",
    "    # non-blocking receives\n",
    "    reqs[0] = comm.Irecv([rbuf1, MPI.INT], source=prev, tag=tag1)\n",
    "    reqs[1] = comm.Irecv([rbuf2, MPI.INT], source=next, tag=tag2)\n",
    "    # non-blocking sends\n",
    "    reqs[2] = comm.Isend([sbuf, MPI.INT], dest=prev, tag=tag2)\n",
    "    reqs[3] = comm.Isend([sbuf, MPI.INT], dest=next, tag=tag1)\n",
    "\n",
    "    # can do some work here if you want, e.g. print current\n",
    "    # state of the receive buffers\n",
    "    print(rbuf1[0], rbuf2[0])\n",
    "    \n",
    "    MPI.Request.Waitall(reqs)\n",
    "    print(\"My rank: {:d} and my neighbors: {:d} {:d}\".format(my_rank, rbuf1[0], rbuf2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4148518944250003456 4153022543877373952\n",
      "4148518944250003456 4153022543877373952\n",
      "4148518944250003456 4153022543877373952\n",
      "My rank: 3 and my neighbors: 2 0\n",
      "4148518944250003456 4153022543877373952\n",
      "My rank: 0 and my neighbors: 3 1\n",
      "My rank: 1 and my neighbors: 0 2\n",
      "My rank: 2 and my neighbors: 1 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'python', 'mpi_example6.py'], returncode=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"python\",\"mpi_example6.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the send and receive function are non-blocking, it doesn't matter which order we execute in. Control is returned to the calling function and execution continues. Hence, we are free to do more work while we wait for the communication to finish.  In this case, we print the current status of the receive buffers. Most likely, you will find that they are still in the unitialized states, but after the `Waitall()` function terminates, they will be updated with the ranks of their neighbors, which we print."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MPI with C\n",
    "\n",
    "At the risk of boring you, I have recreated most of these functions (examples 2, 3, 4, and 6) in C. Compare the MPI implementation `mpi4py` with that of C.  The implementation is logically similar but the syntax is, of course, a little bit different. For example, all of the C codes need to call `MPI_Init()` and `MPI_Finalize()`. The buffers are all pointers in C.\n",
    "\n",
    "Compare `mpi_example6.c` to `mpi_example6.py`. In C we explicitly declare `MPI_Request` objects that we pass by reference to `MPI_ISend()` and `MPI_IRecv()`.  This `MPI_Request` array is then used by `MPI_Waitall()` to determine when the sends and receives have completed. This function can also return an `MPI_STATUS` object, but we don't need this so we send it `MPI_STATUSES_IGNORE` argument that tells MPI not to bother.\n",
    "\n",
    "We can execute the files below, but first we need to make them. I have provided Makefile.mpi for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc -o mpi_example2 mpi_example2.c -O3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ld: warning: dylib (/usr/local/Cellar/open-mpi/5.0.2/lib/libmpi.dylib) was built for newer macOS version (14.0) than being linked (11.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc -o mpi_example3 mpi_example3.c -O3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ld: warning: dylib (/usr/local/Cellar/open-mpi/5.0.2/lib/libmpi.dylib) was built for newer macOS version (14.0) than being linked (11.0)\n",
      "ld: warning: dylib (/usr/local/Cellar/open-mpi/5.0.2/lib/libmpi.dylib) was built for newer macOS version (14.0) than being linked (11.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpicc -o mpi_example4 mpi_example4.c -O3\n",
      "mpicc -o mpi_example6 mpi_example6.c -O3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ld: warning: dylib (/usr/local/Cellar/open-mpi/5.0.2/lib/libmpi.dylib) was built for newer macOS version (14.0) than being linked (11.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['make', '-f', 'Makefile.mpi'], returncode=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"make\",\"-f\",\"Makefile.mpi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try running the examples and compare with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! My rank: 1\n",
      "Hello world! My rank: 2\n",
      "Hello world! My rank: 3\n",
      "Hello world! My rank: 0\n",
      "Number of processes: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'mpi_example2'], returncode=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"mpi_example2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to initialize a: 0.037691\n",
      "Time to initialize b: 0.031069\n",
      "Time to add arrays: 0.037002\n",
      "Average: 33333338333346.222656\n",
      "Time to compute average: 0.012622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'mpi_example3'], returncode=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"mpi_example3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should not be a surprise at this point, but we see that the C code runs much faster than the Python code, for the same number of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My rank: 0 and my neighbors: 3 1\n",
      "My rank: 3 and my neighbors: 2 0\n",
      "My rank: 1 and my neighbors: 0 2\n",
      "My rank: 2 and my neighbors: 1 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'mpi_example4'], returncode=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"mpi_example4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "My rank: 0 and my neighbors: 3 1\n",
      "My rank: 1 and my neighbors: 0 2\n",
      "0 0\n",
      "My rank: 3 and my neighbors: 2 0\n",
      "0 0\n",
      "My rank: 2 and my neighbors: 1 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mpirun', '-np', '4', 'mpi_example6'], returncode=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"mpirun\",\"-np\",\"4\",\"mpi_example6\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OpenMP with C\n",
    "\n",
    "At the risk of boring you even further, I have further recreated a couple of these examples (2 and 3) using OpenMP with C. For these examples, the OpenMP implementation is simpler than the MPI implementation. This is not surprising since we are doing relatively simple problems. If you simply need to spread the operations of four loops accross several processes, this will be easier with OpenMP.  Usually, it is only when you need to code for distributed memory machines or need to do something more sophisticated that MPI becomes the better choice.\n",
    "\n",
    "Note that you may come up against the stack limits when using OpenMP. The easy way around this is to type\n",
    "```\n",
    "ulimit -s ulimited\n",
    "```\n",
    "at the Linux command line. This will set the stack size to unlimited. You can confirm that it was reset by typing\n",
    "```\n",
    "ulimit -a\n",
    "```\n",
    "Again, I have provide a makefile for compiling the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -o omp_example2 omp_example2.c -O3 -fopenmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clang: error: unsupported option '-fopenmp'\n",
      "clang: error: unsupported option '-fopenmp'\n",
      "make: *** [omp_example2] Error 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['make', '-f', 'Makefile.omp'], returncode=2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"make\",\"-f\",\"Makefile.omp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"./omp_example2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"./omp_example3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the execution times of `omp_example3` and `mpi_example3` when run with the same number of threads/processes (e.g. by running `mpirun -np 12 mpi_example3`. I found that they were pretty similar so the simplicity of the OpenMP code wins out here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Student Completion**\n",
    "\n",
    "Now Let's try some slightly more complicated examples, using what we learned above.  I have provided some partially completed code, which you need to finish to perform the desired tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
